% function [nll, grad] = InstanceNegLogLikelihood(X, y, theta, modelParams)
% returns the negative log-likelihood and its gradient, given a CRF with parameters theta,
% on data (X, y). 
%
% Inputs:
% X            Data.                           (numCharacters x numImageFeatures matrix)
%              X(:,1) is all ones, i.e., it encodes the intercept/bias term.
% y            Data labels.                    (numCharacters x 1 vector)
% theta        CRF weights/parameters.         (numParams x 1 vector)
%              These are shared among the various singleton / pairwise features.
% modelParams  Struct with three fields:
%   .numHiddenStates     in our case, set to 26 (26 possible characters)
%   .numObservedStates   in our case, set to 2  (each pixel is either on or off)
%   .lambda              the regularization parameter lambda
%
% Outputs:
% nll          Negative log-likelihood of the data.    (scalar)
% grad         Gradient of nll with respect to theta   (numParams x 1 vector)
%
% Copyright (C) Daphne Koller, Stanford Univerity, 2012

function [nll, grad] = InstanceNegLogLikelihood(X, y, theta, modelParams)

    % featureSet is a struct with two fields:
    %    .numParams - the number of parameters in the CRF (this is not numImageFeatures
    %                 nor numFeatures, because of parameter sharing)
    %    .features  - an array comprising the features in the CRF.
    %
    % Each feature is a binary indicator variable, represented by a struct 
    % with three fields:
    %    .var          - a vector containing the variables in the scope of this feature
    %    .assignment   - the assignment that this indicator variable corresponds to
    %    .paramIdx     - the index in theta that this feature corresponds to
    %
    % For example, if we have:
    %   
    %   feature = struct('var', [2 3], 'assignment', [5 6], 'paramIdx', 8);
    %
    % then feature is an indicator function over X_2 and X_3, which takes on a value of 1
    % if X_2 = 5 and X_3 = 6 (which would be 'e' and 'f'), and 0 otherwise. 
    % Its contribution to the log-likelihood would be theta(8) if it's 1, and 0 otherwise.
    %
    % If you're interested in the implementation details of CRFs, 
    % feel free to read through GenerateAllFeatures.m and the functions it calls!
    % For the purposes of this assignment, though, you don't
    % have to understand how this code works. (It's complicated.)
    
    featureSet = GenerateAllFeatures(X, modelParams);

    % Use the featureSet to calculate nll and grad.
    % This is the main part of the assignment, and it is very tricky - be careful!
    % You might want to code up your own numerical gradient checker to make sure
    % your answers are correct.
    %
    % Hint: you can use CliqueTreeCalibrate to calculate logZ effectively. 
    %       We have halfway-modified CliqueTreeCalibrate; complete our implementation 
    %       if you want to use it to compute logZ.
    
    
    nVar = size(X, 1);
    K = modelParams.numHiddenStates;
    
    factors = repmat(struct('var', [], 'card', [], 'val', []), 1, 2*nVar - 1);
    pairVar2factors = zeros(nVar);
    for i = 1:nVar
        factors(i).var = i;
        factors(i).card = K;
        factors(i).val = zeros(1, K);
        
        if i < nVar
            j = i + nVar;
            pairVar2factors(i, i+1) = j;
            factors(j).var = [i i+1];
            factors(j).card = [K K];
            factors(j).val = zeros(1, K^2);
        end
    end
    
    for i = 1:length(featureSet.features)
        feature = featureSet.features(i);
        if length(feature.var) == 1
            factors(feature.var).val(feature.assignment) = ...
                factors(feature.var).val(feature.assignment) + theta(feature.paramIdx);
        elseif length(feature.var) == 2
            iFct = pairVar2factors(feature.var(1), feature.var(2));
            idx = AssignmentToIndex(feature.assignment, factors(iFct).card);
            factors(iFct).val(idx) = factors(iFct).val(idx) + theta(feature.paramIdx);
        end
    end
    
    for i = 1:length(factors)
        factors(i).val = exp(factors(i).val);
    end
    
    P.cliqueList = repmat(struct('var', [], 'card', [], 'val', []), 1, nVar - 1);
    
    for i = 1:nVar - 1
        iFct = pairVar2factors(i, i+1);
        P.cliqueList(i) = factors(iFct);
        
        if i <= 1
            vars = P.cliqueList(i).var;
        else
            vars = P.cliqueList(i).var(2:end);
        end
      
        for j = vars
            P.cliqueList(i) = FactorProduct(P.cliqueList(i), factors(j));
        end
    end
   
    P.edges = zeros(nVar-1);
    for i = 1:nVar - 2
        P.edges(i, i+1) = 1;
        P.edges(i+1, i) = 1;
    end
    
    [P, logZ] = CliqueTreeCalibrate(P, 0);
    
    weightedFeatureCounts = 0;
    for i = 1:length(featureSet.features)
        feature = featureSet.features(i);
        ind = 0;
        if length(feature.var) == 1
            ind = y(feature.var) == feature.assignment;
            
        elseif length(feature.var) == 2
            ind = y(feature.var(1)) == feature.assignment(1) & ...
                y(feature.var(2)) == feature.assignment(2);
        end
        
        weightedFeatureCounts = weightedFeatureCounts + ...
                theta(feature.paramIdx) * ind;
    end
    
    regularisationCost = 0.5 * modelParams.lambda * sum(theta.^2);
    
    nll = logZ - weightedFeatureCounts + regularisationCost;
    
    M = repmat(struct('var', [], 'card', [], 'val', []), length(P.cliqueList), 1);
    for iVar = 1:nVar
        for iClq = 1:length(P.cliqueList)

            if any(P.cliqueList(iClq).var == iVar)

                marginalisedVars = setdiff(P.cliqueList(iClq).var, iVar);
               
                M(iVar) = FactorMarginalization(P.cliqueList(iClq), marginalisedVars);
                M(iVar).val = M(iVar).val / sum(M(iVar).val);
               
                break;
            end
        end
  
    end
    
    modelFeatureCounts = zeros(size(theta));
    sampleFeatureCounts = zeros(size(theta));
    
    for i = 1:length(featureSet.features)
        feature = featureSet.features(i);
        ind = 0;
        if length(feature.var) == 1
            ind = y(feature.var) == feature.assignment;
                    
            modelFeatureCounts(feature.paramIdx) = ...
                modelFeatureCounts(feature.paramIdx) + ...
                M(feature.var).val(feature.assignment);
        elseif length(feature.var) == 2
            ind = y(feature.var(1)) == feature.assignment(1) & ...
                y(feature.var(2)) == feature.assignment(2);
            
            iClq = feature.var(1);
            idx = AssignmentToIndex(y(feature.var), P.cliqueList(iClq).card);
            modelFeatureCounts(feature.paramIdx) = ...
               modelFeatureCounts(feature.paramIdx) + ...
                P.cliqueList(iClq).val(idx);
        end
        
        sampleFeatureCounts(feature.paramIdx) = ...
            sampleFeatureCounts(feature.paramIdx) + ind;
    end
    
    regularisationGradient = modelParams.lambda * theta;
    
    grad = modelFeatureCounts - sampleFeatureCounts + regularisationGradient;

end
